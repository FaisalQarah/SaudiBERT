{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 11.911230395294291,
  "eval_steps": 500,
  "global_step": 3080000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0,
      "grad_norm": 6.381532669067383,
      "learning_rate": 4.999998710903309e-05,
      "loss": 11.318,
      "step": 1
    },
    {
      "epoch": 0.04,
      "grad_norm": 2.1206963062286377,
      "learning_rate": 4.987109033087245e-05,
      "loss": 6.0554,
      "step": 10000
    },
    {
      "epoch": 0.08,
      "grad_norm": 2.368075370788574,
      "learning_rate": 4.974223222561255e-05,
      "loss": 4.7979,
      "step": 20000
    },
    {
      "epoch": 0.12,
      "grad_norm": 2.461735725402832,
      "learning_rate": 4.961334833841883e-05,
      "loss": 4.3468,
      "step": 30000
    },
    {
      "epoch": 0.15,
      "grad_norm": 2.5674657821655273,
      "learning_rate": 4.948450312412583e-05,
      "loss": 4.082,
      "step": 40000
    },
    {
      "epoch": 0.19,
      "grad_norm": 2.537052631378174,
      "learning_rate": 4.9355657909832846e-05,
      "loss": 3.9078,
      "step": 50000
    },
    {
      "epoch": 0.23,
      "grad_norm": 2.552917957305908,
      "learning_rate": 4.9226799804572946e-05,
      "loss": 3.7757,
      "step": 60000
    },
    {
      "epoch": 0.27,
      "grad_norm": 2.4272916316986084,
      "learning_rate": 4.909795459027995e-05,
      "loss": 3.676,
      "step": 70000
    },
    {
      "epoch": 0.31,
      "grad_norm": 2.4654223918914795,
      "learning_rate": 4.896909648502005e-05,
      "loss": 3.5938,
      "step": 80000
    },
    {
      "epoch": 0.35,
      "grad_norm": 2.660524606704712,
      "learning_rate": 4.8840251270727065e-05,
      "loss": 3.5233,
      "step": 90000
    },
    {
      "epoch": 0.39,
      "grad_norm": 2.5800986289978027,
      "learning_rate": 4.8711393165467165e-05,
      "loss": 3.4679,
      "step": 100000
    },
    {
      "epoch": 0.43,
      "grad_norm": 2.495483160018921,
      "learning_rate": 4.8582535060207265e-05,
      "loss": 3.4167,
      "step": 110000
    },
    {
      "epoch": 0.46,
      "grad_norm": 2.5069963932037354,
      "learning_rate": 4.8453676954947365e-05,
      "loss": 3.3714,
      "step": 120000
    },
    {
      "epoch": 0.5,
      "grad_norm": 2.634019136428833,
      "learning_rate": 4.832481884968746e-05,
      "loss": 3.3319,
      "step": 130000
    },
    {
      "epoch": 0.54,
      "grad_norm": 2.464998245239258,
      "learning_rate": 4.8195947853460646e-05,
      "loss": 3.2976,
      "step": 140000
    },
    {
      "epoch": 0.58,
      "grad_norm": 2.4178707599639893,
      "learning_rate": 4.806707685723383e-05,
      "loss": 3.2654,
      "step": 150000
    },
    {
      "epoch": 0.62,
      "grad_norm": 2.6080141067504883,
      "learning_rate": 4.793820586100702e-05,
      "loss": 3.236,
      "step": 160000
    },
    {
      "epoch": 0.66,
      "grad_norm": 2.6544315814971924,
      "learning_rate": 4.7809360646714027e-05,
      "loss": 3.2104,
      "step": 170000
    },
    {
      "epoch": 0.7,
      "grad_norm": 2.6633081436157227,
      "learning_rate": 4.768051543242104e-05,
      "loss": 3.1831,
      "step": 180000
    },
    {
      "epoch": 0.73,
      "grad_norm": 2.643106698989868,
      "learning_rate": 4.755165732716114e-05,
      "loss": 3.163,
      "step": 190000
    },
    {
      "epoch": 0.77,
      "grad_norm": 2.459101676940918,
      "learning_rate": 4.742281211286815e-05,
      "loss": 3.1436,
      "step": 200000
    },
    {
      "epoch": 0.81,
      "grad_norm": 2.542574405670166,
      "learning_rate": 4.7293966898575165e-05,
      "loss": 3.1211,
      "step": 210000
    },
    {
      "epoch": 0.85,
      "grad_norm": 2.528369903564453,
      "learning_rate": 4.716512168428218e-05,
      "loss": 3.1029,
      "step": 220000
    },
    {
      "epoch": 0.89,
      "grad_norm": 2.4510815143585205,
      "learning_rate": 4.703626357902227e-05,
      "loss": 3.0878,
      "step": 230000
    },
    {
      "epoch": 0.93,
      "grad_norm": 2.675987958908081,
      "learning_rate": 4.690740547376237e-05,
      "loss": 3.0716,
      "step": 240000
    },
    {
      "epoch": 0.97,
      "grad_norm": 2.4895083904266357,
      "learning_rate": 4.677854736850247e-05,
      "loss": 3.0548,
      "step": 250000
    },
    {
      "epoch": 1.01,
      "grad_norm": 2.508035659790039,
      "learning_rate": 4.664968926324257e-05,
      "loss": 3.0379,
      "step": 260000
    },
    {
      "epoch": 1.04,
      "grad_norm": 2.5797290802001953,
      "learning_rate": 4.652083115798267e-05,
      "loss": 3.0224,
      "step": 270000
    },
    {
      "epoch": 1.08,
      "grad_norm": 2.474121570587158,
      "learning_rate": 4.639196016175585e-05,
      "loss": 3.0116,
      "step": 280000
    },
    {
      "epoch": 1.12,
      "grad_norm": 2.550549030303955,
      "learning_rate": 4.626310205649595e-05,
      "loss": 2.9976,
      "step": 290000
    },
    {
      "epoch": 1.16,
      "grad_norm": 2.563823938369751,
      "learning_rate": 4.613421816930223e-05,
      "loss": 2.9863,
      "step": 300000
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.50787615776062,
      "learning_rate": 4.600536006404233e-05,
      "loss": 2.974,
      "step": 310000
    },
    {
      "epoch": 1.24,
      "grad_norm": 2.6312899589538574,
      "learning_rate": 4.587651484974933e-05,
      "loss": 2.9638,
      "step": 320000
    },
    {
      "epoch": 1.28,
      "grad_norm": 2.5924322605133057,
      "learning_rate": 4.574763096255561e-05,
      "loss": 2.9545,
      "step": 330000
    },
    {
      "epoch": 1.31,
      "grad_norm": 2.642179012298584,
      "learning_rate": 4.561878574826262e-05,
      "loss": 2.9449,
      "step": 340000
    },
    {
      "epoch": 1.35,
      "grad_norm": 2.481820583343506,
      "learning_rate": 4.548991475203581e-05,
      "loss": 2.9356,
      "step": 350000
    },
    {
      "epoch": 1.39,
      "grad_norm": 2.5915160179138184,
      "learning_rate": 4.5361043755808996e-05,
      "loss": 2.9272,
      "step": 360000
    },
    {
      "epoch": 1.43,
      "grad_norm": 2.714785575866699,
      "learning_rate": 4.523218565054909e-05,
      "loss": 2.9185,
      "step": 370000
    },
    {
      "epoch": 1.47,
      "grad_norm": 2.6491305828094482,
      "learning_rate": 4.5103314654322276e-05,
      "loss": 2.9112,
      "step": 380000
    },
    {
      "epoch": 1.51,
      "grad_norm": 2.688361883163452,
      "learning_rate": 4.4974443658095464e-05,
      "loss": 2.8996,
      "step": 390000
    },
    {
      "epoch": 1.55,
      "grad_norm": 2.5312700271606445,
      "learning_rate": 4.48456242257363e-05,
      "loss": 2.8947,
      "step": 400000
    },
    {
      "epoch": 1.59,
      "grad_norm": 2.6691203117370605,
      "learning_rate": 4.471674033854257e-05,
      "loss": 2.887,
      "step": 410000
    },
    {
      "epoch": 1.62,
      "grad_norm": 2.468242645263672,
      "learning_rate": 4.458788223328268e-05,
      "loss": 2.8796,
      "step": 420000
    },
    {
      "epoch": 1.66,
      "grad_norm": 2.5768909454345703,
      "learning_rate": 4.445901123705586e-05,
      "loss": 2.8724,
      "step": 430000
    },
    {
      "epoch": 1.7,
      "grad_norm": 2.5607264041900635,
      "learning_rate": 4.433015313179596e-05,
      "loss": 2.8639,
      "step": 440000
    },
    {
      "epoch": 1.74,
      "grad_norm": 2.5702762603759766,
      "learning_rate": 4.420129502653606e-05,
      "loss": 2.8568,
      "step": 450000
    },
    {
      "epoch": 1.78,
      "grad_norm": 2.753458261489868,
      "learning_rate": 4.4072424030309245e-05,
      "loss": 2.8514,
      "step": 460000
    },
    {
      "epoch": 1.82,
      "grad_norm": 2.7910027503967285,
      "learning_rate": 4.394356592504934e-05,
      "loss": 2.8446,
      "step": 470000
    },
    {
      "epoch": 1.86,
      "grad_norm": 2.6404964923858643,
      "learning_rate": 4.381469492882253e-05,
      "loss": 2.8376,
      "step": 480000
    },
    {
      "epoch": 1.89,
      "grad_norm": 2.6894280910491943,
      "learning_rate": 4.3685823932595713e-05,
      "loss": 2.8324,
      "step": 490000
    },
    {
      "epoch": 1.93,
      "grad_norm": 2.807600259780884,
      "learning_rate": 4.3556978718302726e-05,
      "loss": 2.8249,
      "step": 500000
    },
    {
      "epoch": 1.97,
      "grad_norm": 2.6545963287353516,
      "learning_rate": 4.342812061304282e-05,
      "loss": 2.8219,
      "step": 510000
    },
    {
      "epoch": 2.01,
      "grad_norm": 2.7565791606903076,
      "learning_rate": 4.3299249616816014e-05,
      "loss": 2.8127,
      "step": 520000
    },
    {
      "epoch": 2.05,
      "grad_norm": 2.7309985160827637,
      "learning_rate": 4.3170378620589194e-05,
      "loss": 2.8037,
      "step": 530000
    },
    {
      "epoch": 2.09,
      "grad_norm": 2.6734089851379395,
      "learning_rate": 4.3041520515329295e-05,
      "loss": 2.8,
      "step": 540000
    },
    {
      "epoch": 2.13,
      "grad_norm": 2.8645455837249756,
      "learning_rate": 4.291264951910248e-05,
      "loss": 2.794,
      "step": 550000
    },
    {
      "epoch": 2.17,
      "grad_norm": 2.797046422958374,
      "learning_rate": 4.278383008674332e-05,
      "loss": 2.7904,
      "step": 560000
    },
    {
      "epoch": 2.2,
      "grad_norm": 2.82847261428833,
      "learning_rate": 4.2654933308582675e-05,
      "loss": 2.785,
      "step": 570000
    },
    {
      "epoch": 2.24,
      "grad_norm": 2.8019165992736816,
      "learning_rate": 4.252606231235586e-05,
      "loss": 2.7819,
      "step": 580000
    },
    {
      "epoch": 2.28,
      "grad_norm": 2.696074962615967,
      "learning_rate": 4.239719131612905e-05,
      "loss": 2.7764,
      "step": 590000
    },
    {
      "epoch": 2.32,
      "grad_norm": 2.7046563625335693,
      "learning_rate": 4.226834610183606e-05,
      "loss": 2.773,
      "step": 600000
    },
    {
      "epoch": 2.36,
      "grad_norm": 2.7130825519561768,
      "learning_rate": 4.213946221464233e-05,
      "loss": 2.7685,
      "step": 610000
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.7927019596099854,
      "learning_rate": 4.2010578327448606e-05,
      "loss": 2.7635,
      "step": 620000
    },
    {
      "epoch": 2.44,
      "grad_norm": 2.778961658477783,
      "learning_rate": 4.188174600412254e-05,
      "loss": 2.7587,
      "step": 630000
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.7182226181030273,
      "learning_rate": 4.1752862116928806e-05,
      "loss": 2.7548,
      "step": 640000
    },
    {
      "epoch": 2.51,
      "grad_norm": 2.785440683364868,
      "learning_rate": 4.162401690263582e-05,
      "loss": 2.7512,
      "step": 650000
    },
    {
      "epoch": 2.55,
      "grad_norm": 2.9159979820251465,
      "learning_rate": 4.1495145906409006e-05,
      "loss": 2.7482,
      "step": 660000
    },
    {
      "epoch": 2.59,
      "grad_norm": 2.7121965885162354,
      "learning_rate": 4.13662878011491e-05,
      "loss": 2.7427,
      "step": 670000
    },
    {
      "epoch": 2.63,
      "grad_norm": 2.7765676975250244,
      "learning_rate": 4.1237416804922294e-05,
      "loss": 2.7377,
      "step": 680000
    },
    {
      "epoch": 2.67,
      "grad_norm": 2.8434512615203857,
      "learning_rate": 4.11085715906293e-05,
      "loss": 2.7377,
      "step": 690000
    },
    {
      "epoch": 2.71,
      "grad_norm": 2.76558780670166,
      "learning_rate": 4.097968770343557e-05,
      "loss": 2.7333,
      "step": 700000
    },
    {
      "epoch": 2.75,
      "grad_norm": 2.731937885284424,
      "learning_rate": 4.085081670720876e-05,
      "loss": 2.7301,
      "step": 710000
    },
    {
      "epoch": 2.78,
      "grad_norm": 2.892582416534424,
      "learning_rate": 4.07219972748496e-05,
      "loss": 2.726,
      "step": 720000
    },
    {
      "epoch": 2.82,
      "grad_norm": 2.889728307723999,
      "learning_rate": 4.059311338765587e-05,
      "loss": 2.7224,
      "step": 730000
    },
    {
      "epoch": 2.86,
      "grad_norm": 2.8603127002716064,
      "learning_rate": 4.0464293955296706e-05,
      "loss": 2.7192,
      "step": 740000
    },
    {
      "epoch": 2.9,
      "grad_norm": 2.800259590148926,
      "learning_rate": 4.033541006810298e-05,
      "loss": 2.7154,
      "step": 750000
    },
    {
      "epoch": 2.94,
      "grad_norm": 2.8954105377197266,
      "learning_rate": 4.0206526180909256e-05,
      "loss": 2.7101,
      "step": 760000
    },
    {
      "epoch": 2.98,
      "grad_norm": 3.0008273124694824,
      "learning_rate": 4.0077693857583175e-05,
      "loss": 2.7072,
      "step": 770000
    },
    {
      "epoch": 3.02,
      "grad_norm": 2.7593095302581787,
      "learning_rate": 3.994887442522401e-05,
      "loss": 2.7072,
      "step": 780000
    },
    {
      "epoch": 3.06,
      "grad_norm": 3.097625970840454,
      "learning_rate": 3.981996475609646e-05,
      "loss": 2.6949,
      "step": 790000
    },
    {
      "epoch": 3.09,
      "grad_norm": 2.740535020828247,
      "learning_rate": 3.969110665083656e-05,
      "loss": 2.6946,
      "step": 800000
    },
    {
      "epoch": 3.13,
      "grad_norm": 2.9526305198669434,
      "learning_rate": 3.9562248545576656e-05,
      "loss": 2.6918,
      "step": 810000
    },
    {
      "epoch": 3.17,
      "grad_norm": 2.9771251678466797,
      "learning_rate": 3.943336465838293e-05,
      "loss": 2.6875,
      "step": 820000
    },
    {
      "epoch": 3.21,
      "grad_norm": 3.0102577209472656,
      "learning_rate": 3.9304480771189205e-05,
      "loss": 2.6853,
      "step": 830000
    },
    {
      "epoch": 3.25,
      "grad_norm": 2.9867663383483887,
      "learning_rate": 3.917564844786313e-05,
      "loss": 2.6821,
      "step": 840000
    },
    {
      "epoch": 3.29,
      "grad_norm": 2.7795333862304688,
      "learning_rate": 3.904679034260323e-05,
      "loss": 2.6802,
      "step": 850000
    },
    {
      "epoch": 3.33,
      "grad_norm": 3.0124671459198,
      "learning_rate": 3.891791934637641e-05,
      "loss": 2.6772,
      "step": 860000
    },
    {
      "epoch": 3.36,
      "grad_norm": 2.794353485107422,
      "learning_rate": 3.8789074132083424e-05,
      "loss": 2.6741,
      "step": 870000
    },
    {
      "epoch": 3.4,
      "grad_norm": 2.909700632095337,
      "learning_rate": 3.866022891779044e-05,
      "loss": 2.6723,
      "step": 880000
    },
    {
      "epoch": 3.44,
      "grad_norm": 2.7510805130004883,
      "learning_rate": 3.853138370349745e-05,
      "loss": 2.6703,
      "step": 890000
    },
    {
      "epoch": 3.48,
      "grad_norm": 2.957822799682617,
      "learning_rate": 3.8402499816303725e-05,
      "loss": 2.6665,
      "step": 900000
    },
    {
      "epoch": 3.52,
      "grad_norm": 2.9369852542877197,
      "learning_rate": 3.827366749297765e-05,
      "loss": 2.6654,
      "step": 910000
    },
    {
      "epoch": 3.56,
      "grad_norm": 3.0127999782562256,
      "learning_rate": 3.814480938771774e-05,
      "loss": 2.6645,
      "step": 920000
    },
    {
      "epoch": 3.6,
      "grad_norm": 2.9653875827789307,
      "learning_rate": 3.8015964173424756e-05,
      "loss": 2.6619,
      "step": 930000
    },
    {
      "epoch": 3.64,
      "grad_norm": 3.0399117469787598,
      "learning_rate": 3.788706739526412e-05,
      "loss": 2.6586,
      "step": 940000
    },
    {
      "epoch": 3.67,
      "grad_norm": 3.075669050216675,
      "learning_rate": 3.775818350807039e-05,
      "loss": 2.6558,
      "step": 950000
    },
    {
      "epoch": 3.71,
      "grad_norm": 2.7156758308410645,
      "learning_rate": 3.7629389857645056e-05,
      "loss": 2.6572,
      "step": 960000
    },
    {
      "epoch": 3.75,
      "grad_norm": 3.5985991954803467,
      "learning_rate": 3.750051886141824e-05,
      "loss": 3.5259,
      "step": 970000
    },
    {
      "epoch": 3.79,
      "grad_norm": 2.808363676071167,
      "learning_rate": 3.73716220832576e-05,
      "loss": 2.6909,
      "step": 980000
    },
    {
      "epoch": 3.83,
      "grad_norm": 2.8077874183654785,
      "learning_rate": 3.724275108703079e-05,
      "loss": 2.664,
      "step": 990000
    },
    {
      "epoch": 3.87,
      "grad_norm": 3.6062545776367188,
      "learning_rate": 3.711389298177089e-05,
      "loss": 2.6559,
      "step": 1000000
    },
    {
      "epoch": 3.91,
      "grad_norm": 3.2125861644744873,
      "learning_rate": 3.698503487651098e-05,
      "loss": 2.6504,
      "step": 1010000
    },
    {
      "epoch": 3.94,
      "grad_norm": 2.96240234375,
      "learning_rate": 3.685617677125108e-05,
      "loss": 2.6513,
      "step": 1020000
    },
    {
      "epoch": 3.98,
      "grad_norm": 2.9129388332366943,
      "learning_rate": 3.672730577502427e-05,
      "loss": 2.6457,
      "step": 1030000
    },
    {
      "epoch": 4.02,
      "grad_norm": 2.8948028087615967,
      "learning_rate": 3.6598434778797455e-05,
      "loss": 2.6382,
      "step": 1040000
    },
    {
      "epoch": 4.06,
      "grad_norm": 2.8889665603637695,
      "learning_rate": 3.646958956450446e-05,
      "loss": 2.6342,
      "step": 1050000
    },
    {
      "epoch": 4.1,
      "grad_norm": 2.7647414207458496,
      "learning_rate": 3.6340718568277655e-05,
      "loss": 2.6294,
      "step": 1060000
    },
    {
      "epoch": 4.14,
      "grad_norm": 2.968823194503784,
      "learning_rate": 3.621182179011701e-05,
      "loss": 2.6276,
      "step": 1070000
    },
    {
      "epoch": 4.18,
      "grad_norm": 2.9329586029052734,
      "learning_rate": 3.608298946679094e-05,
      "loss": 2.627,
      "step": 1080000
    },
    {
      "epoch": 4.22,
      "grad_norm": 2.902411699295044,
      "learning_rate": 3.5954118470564124e-05,
      "loss": 2.624,
      "step": 1090000
    },
    {
      "epoch": 4.25,
      "grad_norm": 2.8417935371398926,
      "learning_rate": 3.582526036530422e-05,
      "loss": 2.6218,
      "step": 1100000
    },
    {
      "epoch": 4.29,
      "grad_norm": 3.107492685317993,
      "learning_rate": 3.569638936907741e-05,
      "loss": 2.6214,
      "step": 1110000
    },
    {
      "epoch": 4.33,
      "grad_norm": 2.748335838317871,
      "learning_rate": 3.5567531263817505e-05,
      "loss": 2.6188,
      "step": 1120000
    },
    {
      "epoch": 4.37,
      "grad_norm": 2.9352238178253174,
      "learning_rate": 3.5438673158557605e-05,
      "loss": 2.618,
      "step": 1130000
    },
    {
      "epoch": 4.41,
      "grad_norm": 2.985463857650757,
      "learning_rate": 3.530978927136388e-05,
      "loss": 2.6154,
      "step": 1140000
    },
    {
      "epoch": 4.45,
      "grad_norm": 2.9371418952941895,
      "learning_rate": 3.518091827513707e-05,
      "loss": 2.6108,
      "step": 1150000
    },
    {
      "epoch": 4.49,
      "grad_norm": 3.0834195613861084,
      "learning_rate": 3.505207306084407e-05,
      "loss": 2.6094,
      "step": 1160000
    },
    {
      "epoch": 4.52,
      "grad_norm": 2.877213716506958,
      "learning_rate": 3.492321495558418e-05,
      "loss": 2.6085,
      "step": 1170000
    },
    {
      "epoch": 4.56,
      "grad_norm": 3.1641488075256348,
      "learning_rate": 3.479435685032427e-05,
      "loss": 2.605,
      "step": 1180000
    },
    {
      "epoch": 4.6,
      "grad_norm": 3.0917696952819824,
      "learning_rate": 3.466548585409746e-05,
      "loss": 2.6054,
      "step": 1190000
    },
    {
      "epoch": 4.64,
      "grad_norm": 2.9926090240478516,
      "learning_rate": 3.453662774883756e-05,
      "loss": 2.6047,
      "step": 1200000
    },
    {
      "epoch": 4.68,
      "grad_norm": 3.1165122985839844,
      "learning_rate": 3.440776964357766e-05,
      "loss": 2.6016,
      "step": 1210000
    },
    {
      "epoch": 4.72,
      "grad_norm": 3.5662155151367188,
      "learning_rate": 3.427889864735084e-05,
      "loss": 2.5995,
      "step": 1220000
    },
    {
      "epoch": 4.76,
      "grad_norm": 3.1813127994537354,
      "learning_rate": 3.415004054209094e-05,
      "loss": 2.5983,
      "step": 1230000
    },
    {
      "epoch": 4.8,
      "grad_norm": 3.0858590602874756,
      "learning_rate": 3.402116954586413e-05,
      "loss": 2.598,
      "step": 1240000
    },
    {
      "epoch": 4.83,
      "grad_norm": 2.9742963314056396,
      "learning_rate": 3.3892298549637317e-05,
      "loss": 2.5951,
      "step": 1250000
    },
    {
      "epoch": 4.87,
      "grad_norm": 2.8759970664978027,
      "learning_rate": 3.376344044437742e-05,
      "loss": 2.5932,
      "step": 1260000
    },
    {
      "epoch": 4.91,
      "grad_norm": 2.9735264778137207,
      "learning_rate": 3.363458233911751e-05,
      "loss": 2.5914,
      "step": 1270000
    },
    {
      "epoch": 4.95,
      "grad_norm": 3.1311378479003906,
      "learning_rate": 3.3505698451923785e-05,
      "loss": 2.5892,
      "step": 1280000
    },
    {
      "epoch": 4.99,
      "grad_norm": 2.941310405731201,
      "learning_rate": 3.3376840346663885e-05,
      "loss": 2.5866,
      "step": 1290000
    },
    {
      "epoch": 5.03,
      "grad_norm": 2.964764356613159,
      "learning_rate": 3.324798224140398e-05,
      "loss": 2.5803,
      "step": 1300000
    },
    {
      "epoch": 5.07,
      "grad_norm": 2.9160892963409424,
      "learning_rate": 3.311911124517717e-05,
      "loss": 2.5771,
      "step": 1310000
    },
    {
      "epoch": 5.1,
      "grad_norm": 3.104168176651001,
      "learning_rate": 3.299026603088418e-05,
      "loss": 2.5754,
      "step": 1320000
    },
    {
      "epoch": 5.14,
      "grad_norm": 3.1793100833892822,
      "learning_rate": 3.286138214369045e-05,
      "loss": 2.5739,
      "step": 1330000
    },
    {
      "epoch": 5.18,
      "grad_norm": 3.058725118637085,
      "learning_rate": 3.273251114746364e-05,
      "loss": 2.5723,
      "step": 1340000
    },
    {
      "epoch": 5.22,
      "grad_norm": 3.0404767990112305,
      "learning_rate": 3.2603665933170653e-05,
      "loss": 2.573,
      "step": 1350000
    },
    {
      "epoch": 5.26,
      "grad_norm": 3.0515799522399902,
      "learning_rate": 3.2474794936943834e-05,
      "loss": 2.5703,
      "step": 1360000
    },
    {
      "epoch": 5.3,
      "grad_norm": 2.9365034103393555,
      "learning_rate": 3.234594972265085e-05,
      "loss": 2.5699,
      "step": 1370000
    },
    {
      "epoch": 5.34,
      "grad_norm": 3.230992078781128,
      "learning_rate": 3.221709161739095e-05,
      "loss": 2.5685,
      "step": 1380000
    },
    {
      "epoch": 5.38,
      "grad_norm": 3.048720121383667,
      "learning_rate": 3.208823351213105e-05,
      "loss": 2.5671,
      "step": 1390000
    },
    {
      "epoch": 5.41,
      "grad_norm": 3.002091407775879,
      "learning_rate": 3.195937540687115e-05,
      "loss": 2.5649,
      "step": 1400000
    },
    {
      "epoch": 5.45,
      "grad_norm": 3.0786585807800293,
      "learning_rate": 3.183050441064433e-05,
      "loss": 2.5632,
      "step": 1410000
    },
    {
      "epoch": 5.49,
      "grad_norm": 2.9854071140289307,
      "learning_rate": 3.170165919635134e-05,
      "loss": 2.564,
      "step": 1420000
    },
    {
      "epoch": 5.53,
      "grad_norm": 3.1634116172790527,
      "learning_rate": 3.157278820012453e-05,
      "loss": 2.5608,
      "step": 1430000
    },
    {
      "epoch": 5.57,
      "grad_norm": 3.0326905250549316,
      "learning_rate": 3.144393009486463e-05,
      "loss": 2.5604,
      "step": 1440000
    },
    {
      "epoch": 5.61,
      "grad_norm": 2.912137269973755,
      "learning_rate": 3.131508488057164e-05,
      "loss": 2.5573,
      "step": 1450000
    },
    {
      "epoch": 5.65,
      "grad_norm": 3.0979037284851074,
      "learning_rate": 3.118621388434482e-05,
      "loss": 2.5576,
      "step": 1460000
    },
    {
      "epoch": 5.68,
      "grad_norm": 3.295593023300171,
      "learning_rate": 3.1057368670051834e-05,
      "loss": 2.5566,
      "step": 1470000
    },
    {
      "epoch": 5.72,
      "grad_norm": 3.1175358295440674,
      "learning_rate": 3.092848478285811e-05,
      "loss": 2.555,
      "step": 1480000
    },
    {
      "epoch": 5.76,
      "grad_norm": 3.1961305141448975,
      "learning_rate": 3.079961378663129e-05,
      "loss": 2.5519,
      "step": 1490000
    },
    {
      "epoch": 5.8,
      "grad_norm": 3.2275586128234863,
      "learning_rate": 3.0670742790404484e-05,
      "loss": 2.5509,
      "step": 1500000
    },
    {
      "epoch": 5.84,
      "grad_norm": 3.146303176879883,
      "learning_rate": 3.0541871794177665e-05,
      "loss": 2.5514,
      "step": 1510000
    },
    {
      "epoch": 5.88,
      "grad_norm": 2.9979844093322754,
      "learning_rate": 3.0413052361818506e-05,
      "loss": 2.5484,
      "step": 1520000
    },
    {
      "epoch": 5.92,
      "grad_norm": 3.086001396179199,
      "learning_rate": 3.028416847462478e-05,
      "loss": 2.5452,
      "step": 1530000
    },
    {
      "epoch": 5.96,
      "grad_norm": 2.953791379928589,
      "learning_rate": 3.015532326033179e-05,
      "loss": 2.5429,
      "step": 1540000
    },
    {
      "epoch": 5.99,
      "grad_norm": 3.0217175483703613,
      "learning_rate": 3.0026465155071887e-05,
      "loss": 2.5442,
      "step": 1550000
    },
    {
      "epoch": 6.03,
      "grad_norm": 3.046329975128174,
      "learning_rate": 2.9897632831745816e-05,
      "loss": 2.5372,
      "step": 1560000
    },
    {
      "epoch": 6.07,
      "grad_norm": 3.055058002471924,
      "learning_rate": 2.9768748944552084e-05,
      "loss": 2.5353,
      "step": 1570000
    },
    {
      "epoch": 6.11,
      "grad_norm": 3.185265064239502,
      "learning_rate": 2.963987794832527e-05,
      "loss": 2.5346,
      "step": 1580000
    },
    {
      "epoch": 6.15,
      "grad_norm": 3.3198447227478027,
      "learning_rate": 2.9511045624999193e-05,
      "loss": 2.5348,
      "step": 1590000
    },
    {
      "epoch": 6.19,
      "grad_norm": 3.07922101020813,
      "learning_rate": 2.9382161737805468e-05,
      "loss": 2.5343,
      "step": 1600000
    },
    {
      "epoch": 6.23,
      "grad_norm": 3.0249123573303223,
      "learning_rate": 2.925331652351248e-05,
      "loss": 2.5334,
      "step": 1610000
    },
    {
      "epoch": 6.26,
      "grad_norm": 3.1425509452819824,
      "learning_rate": 2.9124445527285665e-05,
      "loss": 2.5314,
      "step": 1620000
    },
    {
      "epoch": 6.3,
      "grad_norm": 2.999638795852661,
      "learning_rate": 2.899557453105885e-05,
      "loss": 2.5304,
      "step": 1630000
    },
    {
      "epoch": 6.34,
      "grad_norm": 3.136577606201172,
      "learning_rate": 2.886670353483204e-05,
      "loss": 2.529,
      "step": 1640000
    },
    {
      "epoch": 6.38,
      "grad_norm": 3.1833808422088623,
      "learning_rate": 2.8737819647638315e-05,
      "loss": 2.5281,
      "step": 1650000
    },
    {
      "epoch": 6.42,
      "grad_norm": 2.967899799346924,
      "learning_rate": 2.8608935760444583e-05,
      "loss": 2.5246,
      "step": 1660000
    },
    {
      "epoch": 6.46,
      "grad_norm": 3.0674800872802734,
      "learning_rate": 2.8480090546151596e-05,
      "loss": 2.5244,
      "step": 1670000
    },
    {
      "epoch": 6.5,
      "grad_norm": 3.122684955596924,
      "learning_rate": 2.8351206658957867e-05,
      "loss": 2.5223,
      "step": 1680000
    },
    {
      "epoch": 6.54,
      "grad_norm": 3.0649142265319824,
      "learning_rate": 2.8222387226598705e-05,
      "loss": 2.5232,
      "step": 1690000
    },
    {
      "epoch": 6.57,
      "grad_norm": 3.0442910194396973,
      "learning_rate": 2.809350333940498e-05,
      "loss": 2.5216,
      "step": 1700000
    },
    {
      "epoch": 6.61,
      "grad_norm": 3.2312262058258057,
      "learning_rate": 2.7964658125111993e-05,
      "loss": 2.5202,
      "step": 1710000
    },
    {
      "epoch": 6.65,
      "grad_norm": 3.2760837078094482,
      "learning_rate": 2.7835800019852093e-05,
      "loss": 2.5185,
      "step": 1720000
    },
    {
      "epoch": 6.69,
      "grad_norm": 3.0800976753234863,
      "learning_rate": 2.7706929023625277e-05,
      "loss": 2.5168,
      "step": 1730000
    },
    {
      "epoch": 6.73,
      "grad_norm": 3.0953640937805176,
      "learning_rate": 2.757805802739846e-05,
      "loss": 2.5168,
      "step": 1740000
    },
    {
      "epoch": 6.77,
      "grad_norm": 27.8433837890625,
      "learning_rate": 2.7449212813105474e-05,
      "loss": 2.5138,
      "step": 1750000
    },
    {
      "epoch": 6.81,
      "grad_norm": 3.1463799476623535,
      "learning_rate": 2.732032892591175e-05,
      "loss": 2.5157,
      "step": 1760000
    },
    {
      "epoch": 6.85,
      "grad_norm": 3.157773733139038,
      "learning_rate": 2.7191445038718023e-05,
      "loss": 2.5132,
      "step": 1770000
    },
    {
      "epoch": 6.88,
      "grad_norm": 3.0448927879333496,
      "learning_rate": 2.7062561151524295e-05,
      "loss": 2.5121,
      "step": 1780000
    },
    {
      "epoch": 6.92,
      "grad_norm": 3.320538282394409,
      "learning_rate": 2.6933690155297482e-05,
      "loss": 2.5095,
      "step": 1790000
    },
    {
      "epoch": 6.96,
      "grad_norm": 3.1755669116973877,
      "learning_rate": 2.680484494100449e-05,
      "loss": 2.5095,
      "step": 1800000
    },
    {
      "epoch": 7.0,
      "grad_norm": 3.109607696533203,
      "learning_rate": 2.6676012617678414e-05,
      "loss": 2.5094,
      "step": 1810000
    },
    {
      "epoch": 7.04,
      "grad_norm": 3.206146001815796,
      "learning_rate": 2.6547167403385426e-05,
      "loss": 2.5015,
      "step": 1820000
    },
    {
      "epoch": 7.08,
      "grad_norm": 3.103619337081909,
      "learning_rate": 2.641829640715861e-05,
      "loss": 2.5008,
      "step": 1830000
    },
    {
      "epoch": 7.12,
      "grad_norm": 3.304959297180176,
      "learning_rate": 2.6289412519964885e-05,
      "loss": 2.4998,
      "step": 1840000
    },
    {
      "epoch": 7.15,
      "grad_norm": 3.6546099185943604,
      "learning_rate": 2.6160554414704985e-05,
      "loss": 2.5008,
      "step": 1850000
    },
    {
      "epoch": 7.19,
      "grad_norm": 3.0604681968688965,
      "learning_rate": 2.6031722091378907e-05,
      "loss": 2.4999,
      "step": 1860000
    },
    {
      "epoch": 7.23,
      "grad_norm": 3.3865740299224854,
      "learning_rate": 2.590282531321827e-05,
      "loss": 2.4985,
      "step": 1870000
    },
    {
      "epoch": 7.27,
      "grad_norm": 3.3395442962646484,
      "learning_rate": 2.5773980098925282e-05,
      "loss": 2.4981,
      "step": 1880000
    },
    {
      "epoch": 7.31,
      "grad_norm": 3.286437749862671,
      "learning_rate": 2.5645109102698466e-05,
      "loss": 2.4968,
      "step": 1890000
    },
    {
      "epoch": 7.35,
      "grad_norm": 2.9534037113189697,
      "learning_rate": 2.551622521550474e-05,
      "loss": 2.4962,
      "step": 1900000
    },
    {
      "epoch": 7.39,
      "grad_norm": 3.077960252761841,
      "learning_rate": 2.5387354219277925e-05,
      "loss": 2.4938,
      "step": 1910000
    },
    {
      "epoch": 7.43,
      "grad_norm": 3.2909929752349854,
      "learning_rate": 2.52584703320842e-05,
      "loss": 2.4912,
      "step": 1920000
    },
    {
      "epoch": 7.46,
      "grad_norm": 3.0313024520874023,
      "learning_rate": 2.51296122268243e-05,
      "loss": 2.4911,
      "step": 1930000
    },
    {
      "epoch": 7.5,
      "grad_norm": 3.1144065856933594,
      "learning_rate": 2.5000754121564397e-05,
      "loss": 2.4908,
      "step": 1940000
    },
    {
      "epoch": 7.54,
      "grad_norm": 3.17419695854187,
      "learning_rate": 2.4871896016304497e-05,
      "loss": 2.4896,
      "step": 1950000
    },
    {
      "epoch": 7.58,
      "grad_norm": 3.090000629425049,
      "learning_rate": 2.474302502007768e-05,
      "loss": 2.4873,
      "step": 1960000
    },
    {
      "epoch": 7.62,
      "grad_norm": 3.3415465354919434,
      "learning_rate": 2.461416691481778e-05,
      "loss": 2.4864,
      "step": 1970000
    },
    {
      "epoch": 7.66,
      "grad_norm": 3.1867029666900635,
      "learning_rate": 2.4485321700524794e-05,
      "loss": 2.4856,
      "step": 1980000
    },
    {
      "epoch": 7.7,
      "grad_norm": 3.2778003215789795,
      "learning_rate": 2.4356437813331065e-05,
      "loss": 2.4862,
      "step": 1990000
    },
    {
      "epoch": 7.73,
      "grad_norm": 3.1872308254241943,
      "learning_rate": 2.4227566817104253e-05,
      "loss": 2.4846,
      "step": 2000000
    },
    {
      "epoch": 7.77,
      "grad_norm": 3.1750712394714355,
      "learning_rate": 2.4098682929910524e-05,
      "loss": 2.4843,
      "step": 2010000
    },
    {
      "epoch": 7.81,
      "grad_norm": 3.0857629776000977,
      "learning_rate": 2.3969837715617534e-05,
      "loss": 2.4822,
      "step": 2020000
    },
    {
      "epoch": 7.85,
      "grad_norm": 3.2549662590026855,
      "learning_rate": 2.384095382842381e-05,
      "loss": 2.4793,
      "step": 2030000
    },
    {
      "epoch": 7.89,
      "grad_norm": 3.337470531463623,
      "learning_rate": 2.371210861413082e-05,
      "loss": 2.4802,
      "step": 2040000
    },
    {
      "epoch": 7.93,
      "grad_norm": 3.145444631576538,
      "learning_rate": 2.358326339983783e-05,
      "loss": 2.479,
      "step": 2050000
    },
    {
      "epoch": 7.97,
      "grad_norm": 3.354327917098999,
      "learning_rate": 2.3454392403611018e-05,
      "loss": 2.478,
      "step": 2060000
    },
    {
      "epoch": 8.01,
      "grad_norm": 3.1971659660339355,
      "learning_rate": 2.3325521407384206e-05,
      "loss": 2.4768,
      "step": 2070000
    },
    {
      "epoch": 8.04,
      "grad_norm": 3.077310800552368,
      "learning_rate": 2.3196650411157393e-05,
      "loss": 2.47,
      "step": 2080000
    },
    {
      "epoch": 8.08,
      "grad_norm": 3.1938021183013916,
      "learning_rate": 2.3067779414930577e-05,
      "loss": 2.4692,
      "step": 2090000
    },
    {
      "epoch": 8.12,
      "grad_norm": 3.2865288257598877,
      "learning_rate": 2.2938921309670674e-05,
      "loss": 2.4693,
      "step": 2100000
    },
    {
      "epoch": 8.16,
      "grad_norm": 3.2896695137023926,
      "learning_rate": 2.2810063204410774e-05,
      "loss": 2.4686,
      "step": 2110000
    },
    {
      "epoch": 8.2,
      "grad_norm": 3.1974377632141113,
      "learning_rate": 2.268119220818396e-05,
      "loss": 2.4689,
      "step": 2120000
    },
    {
      "epoch": 8.24,
      "grad_norm": 3.3326308727264404,
      "learning_rate": 2.2552334102924058e-05,
      "loss": 2.4669,
      "step": 2130000
    },
    {
      "epoch": 8.28,
      "grad_norm": 3.323216438293457,
      "learning_rate": 2.242343732476342e-05,
      "loss": 2.4655,
      "step": 2140000
    },
    {
      "epoch": 8.31,
      "grad_norm": 3.3018126487731934,
      "learning_rate": 2.2294605001437342e-05,
      "loss": 2.4646,
      "step": 2150000
    },
    {
      "epoch": 8.35,
      "grad_norm": 3.3774096965789795,
      "learning_rate": 2.2165759787144355e-05,
      "loss": 2.4637,
      "step": 2160000
    },
    {
      "epoch": 8.39,
      "grad_norm": 3.184365749359131,
      "learning_rate": 2.2036901681884452e-05,
      "loss": 2.4618,
      "step": 2170000
    },
    {
      "epoch": 8.43,
      "grad_norm": 3.2518510818481445,
      "learning_rate": 2.1908017794690727e-05,
      "loss": 2.4618,
      "step": 2180000
    },
    {
      "epoch": 8.47,
      "grad_norm": 3.3296098709106445,
      "learning_rate": 2.1779133907497e-05,
      "loss": 2.4627,
      "step": 2190000
    },
    {
      "epoch": 8.51,
      "grad_norm": 3.1434078216552734,
      "learning_rate": 2.1650288693204014e-05,
      "loss": 2.4611,
      "step": 2200000
    },
    {
      "epoch": 8.55,
      "grad_norm": 3.410243511199951,
      "learning_rate": 2.1521404806010286e-05,
      "loss": 2.4592,
      "step": 2210000
    },
    {
      "epoch": 8.59,
      "grad_norm": 3.2934114933013916,
      "learning_rate": 2.1392533809783473e-05,
      "loss": 2.4589,
      "step": 2220000
    },
    {
      "epoch": 8.62,
      "grad_norm": 3.4312448501586914,
      "learning_rate": 2.1263688595490483e-05,
      "loss": 2.4584,
      "step": 2230000
    },
    {
      "epoch": 8.66,
      "grad_norm": 3.3087170124053955,
      "learning_rate": 2.1134843381197495e-05,
      "loss": 2.4569,
      "step": 2240000
    },
    {
      "epoch": 8.7,
      "grad_norm": 3.4352591037750244,
      "learning_rate": 2.1005946603036854e-05,
      "loss": 2.4556,
      "step": 2250000
    },
    {
      "epoch": 8.74,
      "grad_norm": 3.5978989601135254,
      "learning_rate": 2.0877088497776954e-05,
      "loss": 2.4558,
      "step": 2260000
    },
    {
      "epoch": 8.78,
      "grad_norm": 3.3699216842651367,
      "learning_rate": 2.0748243283483964e-05,
      "loss": 2.4537,
      "step": 2270000
    },
    {
      "epoch": 8.82,
      "grad_norm": 3.283841371536255,
      "learning_rate": 2.061935939629024e-05,
      "loss": 2.4546,
      "step": 2280000
    },
    {
      "epoch": 8.86,
      "grad_norm": 3.049647331237793,
      "learning_rate": 2.049050129103034e-05,
      "loss": 2.4528,
      "step": 2290000
    },
    {
      "epoch": 8.89,
      "grad_norm": 3.2500860691070557,
      "learning_rate": 2.0361643185770435e-05,
      "loss": 2.4539,
      "step": 2300000
    },
    {
      "epoch": 8.93,
      "grad_norm": 3.4084579944610596,
      "learning_rate": 2.0232772189543623e-05,
      "loss": 2.4523,
      "step": 2310000
    },
    {
      "epoch": 8.97,
      "grad_norm": 3.3977551460266113,
      "learning_rate": 2.0103939866217548e-05,
      "loss": 2.4496,
      "step": 2320000
    },
    {
      "epoch": 9.01,
      "grad_norm": 3.3850505352020264,
      "learning_rate": 1.997505597902382e-05,
      "loss": 2.4479,
      "step": 2330000
    },
    {
      "epoch": 9.05,
      "grad_norm": 3.528064012527466,
      "learning_rate": 1.984619787376392e-05,
      "loss": 2.4415,
      "step": 2340000
    },
    {
      "epoch": 9.09,
      "grad_norm": 3.3169007301330566,
      "learning_rate": 1.9717326877537104e-05,
      "loss": 2.4429,
      "step": 2350000
    },
    {
      "epoch": 9.13,
      "grad_norm": 3.4246697425842285,
      "learning_rate": 1.958845588131029e-05,
      "loss": 2.4434,
      "step": 2360000
    },
    {
      "epoch": 9.17,
      "grad_norm": 3.5705697536468506,
      "learning_rate": 1.9459597776050388e-05,
      "loss": 2.4405,
      "step": 2370000
    },
    {
      "epoch": 9.2,
      "grad_norm": 3.2429282665252686,
      "learning_rate": 1.9330765452724313e-05,
      "loss": 2.4403,
      "step": 2380000
    },
    {
      "epoch": 9.24,
      "grad_norm": 3.2656397819519043,
      "learning_rate": 1.9201894456497497e-05,
      "loss": 2.4421,
      "step": 2390000
    },
    {
      "epoch": 9.28,
      "grad_norm": 3.6163177490234375,
      "learning_rate": 1.907304924220451e-05,
      "loss": 2.4386,
      "step": 2400000
    },
    {
      "epoch": 9.32,
      "grad_norm": 3.5148541927337646,
      "learning_rate": 1.8944165355010785e-05,
      "loss": 2.4393,
      "step": 2410000
    },
    {
      "epoch": 9.36,
      "grad_norm": 3.3997273445129395,
      "learning_rate": 1.881528146781706e-05,
      "loss": 2.4383,
      "step": 2420000
    },
    {
      "epoch": 9.4,
      "grad_norm": 3.2928054332733154,
      "learning_rate": 1.8686423362557156e-05,
      "loss": 2.4382,
      "step": 2430000
    },
    {
      "epoch": 9.44,
      "grad_norm": 3.503671407699585,
      "learning_rate": 1.855753947536343e-05,
      "loss": 2.436,
      "step": 2440000
    },
    {
      "epoch": 9.47,
      "grad_norm": 3.363469123840332,
      "learning_rate": 1.8428681370103528e-05,
      "loss": 2.4356,
      "step": 2450000
    },
    {
      "epoch": 9.51,
      "grad_norm": 3.6115076541900635,
      "learning_rate": 1.8299810373876712e-05,
      "loss": 2.4357,
      "step": 2460000
    },
    {
      "epoch": 9.55,
      "grad_norm": 3.439655065536499,
      "learning_rate": 1.8170926486682987e-05,
      "loss": 2.4336,
      "step": 2470000
    },
    {
      "epoch": 9.59,
      "grad_norm": 3.4758055210113525,
      "learning_rate": 1.804208127239e-05,
      "loss": 2.4338,
      "step": 2480000
    },
    {
      "epoch": 9.63,
      "grad_norm": 3.288316011428833,
      "learning_rate": 1.7913210276163184e-05,
      "loss": 2.4323,
      "step": 2490000
    },
    {
      "epoch": 9.67,
      "grad_norm": 3.2155046463012695,
      "learning_rate": 1.778433927993637e-05,
      "loss": 2.4325,
      "step": 2500000
    },
    {
      "epoch": 9.71,
      "grad_norm": 3.4872522354125977,
      "learning_rate": 1.7655481174676468e-05,
      "loss": 2.4309,
      "step": 2510000
    },
    {
      "epoch": 9.75,
      "grad_norm": 3.280182361602783,
      "learning_rate": 1.7526623069416568e-05,
      "loss": 2.4292,
      "step": 2520000
    },
    {
      "epoch": 9.78,
      "grad_norm": 3.4504904747009277,
      "learning_rate": 1.7397764964156668e-05,
      "loss": 2.4281,
      "step": 2530000
    },
    {
      "epoch": 9.82,
      "grad_norm": 3.5356695652008057,
      "learning_rate": 1.7268893967929852e-05,
      "loss": 2.4266,
      "step": 2540000
    },
    {
      "epoch": 9.86,
      "grad_norm": 3.380032539367676,
      "learning_rate": 1.7140035862669952e-05,
      "loss": 2.4258,
      "step": 2550000
    },
    {
      "epoch": 9.9,
      "grad_norm": 3.6337220668792725,
      "learning_rate": 1.701119064837696e-05,
      "loss": 2.4288,
      "step": 2560000
    },
    {
      "epoch": 9.94,
      "grad_norm": 3.3167779445648193,
      "learning_rate": 1.6882306761183237e-05,
      "loss": 2.4261,
      "step": 2570000
    },
    {
      "epoch": 9.98,
      "grad_norm": 3.4438343048095703,
      "learning_rate": 1.6753435764956424e-05,
      "loss": 2.4258,
      "step": 2580000
    },
    {
      "epoch": 10.02,
      "grad_norm": 3.371340751647949,
      "learning_rate": 1.662457765969652e-05,
      "loss": 2.4225,
      "step": 2590000
    },
    {
      "epoch": 10.05,
      "grad_norm": 3.39367413520813,
      "learning_rate": 1.6495732445403533e-05,
      "loss": 2.4185,
      "step": 2600000
    },
    {
      "epoch": 10.09,
      "grad_norm": 3.626835823059082,
      "learning_rate": 1.636687434014363e-05,
      "loss": 2.4177,
      "step": 2610000
    },
    {
      "epoch": 10.13,
      "grad_norm": 3.3204896450042725,
      "learning_rate": 1.623801623488373e-05,
      "loss": 2.4182,
      "step": 2620000
    },
    {
      "epoch": 10.17,
      "grad_norm": 3.508037567138672,
      "learning_rate": 1.6109132347690005e-05,
      "loss": 2.4154,
      "step": 2630000
    },
    {
      "epoch": 10.21,
      "grad_norm": 3.4004859924316406,
      "learning_rate": 1.5980261351463193e-05,
      "loss": 2.4172,
      "step": 2640000
    },
    {
      "epoch": 10.25,
      "grad_norm": 3.3708651065826416,
      "learning_rate": 1.5851429028137115e-05,
      "loss": 2.4162,
      "step": 2650000
    },
    {
      "epoch": 10.29,
      "grad_norm": 3.396789073944092,
      "learning_rate": 1.5722570922877215e-05,
      "loss": 2.4147,
      "step": 2660000
    },
    {
      "epoch": 10.33,
      "grad_norm": 3.5541019439697266,
      "learning_rate": 1.5593725708584224e-05,
      "loss": 2.4141,
      "step": 2670000
    },
    {
      "epoch": 10.36,
      "grad_norm": 3.4882829189300537,
      "learning_rate": 1.5464841821390495e-05,
      "loss": 2.4147,
      "step": 2680000
    },
    {
      "epoch": 10.4,
      "grad_norm": 3.4063801765441895,
      "learning_rate": 1.533595793419677e-05,
      "loss": 2.4126,
      "step": 2690000
    },
    {
      "epoch": 10.44,
      "grad_norm": 3.463912010192871,
      "learning_rate": 1.5207086937969958e-05,
      "loss": 2.4093,
      "step": 2700000
    },
    {
      "epoch": 10.48,
      "grad_norm": 3.3545310497283936,
      "learning_rate": 1.5078215941743144e-05,
      "loss": 2.4125,
      "step": 2710000
    },
    {
      "epoch": 10.52,
      "grad_norm": 3.654376983642578,
      "learning_rate": 1.4949344945516331e-05,
      "loss": 2.4107,
      "step": 2720000
    },
    {
      "epoch": 10.56,
      "grad_norm": 3.4114062786102295,
      "learning_rate": 1.4820499731223342e-05,
      "loss": 2.4102,
      "step": 2730000
    },
    {
      "epoch": 10.6,
      "grad_norm": 3.433382034301758,
      "learning_rate": 1.4691654516930353e-05,
      "loss": 2.4101,
      "step": 2740000
    },
    {
      "epoch": 10.64,
      "grad_norm": 3.5820817947387695,
      "learning_rate": 1.4562783520703537e-05,
      "loss": 2.4075,
      "step": 2750000
    },
    {
      "epoch": 10.67,
      "grad_norm": 3.408388614654541,
      "learning_rate": 1.4433912524476725e-05,
      "loss": 2.407,
      "step": 2760000
    },
    {
      "epoch": 10.71,
      "grad_norm": 3.6569430828094482,
      "learning_rate": 1.4305028637282996e-05,
      "loss": 2.4082,
      "step": 2770000
    },
    {
      "epoch": 10.75,
      "grad_norm": 3.3861641883850098,
      "learning_rate": 1.4176157641056184e-05,
      "loss": 2.4058,
      "step": 2780000
    },
    {
      "epoch": 10.79,
      "grad_norm": 3.5626070499420166,
      "learning_rate": 1.4047312426763196e-05,
      "loss": 2.405,
      "step": 2790000
    },
    {
      "epoch": 10.83,
      "grad_norm": 3.476039171218872,
      "learning_rate": 1.3918454321503293e-05,
      "loss": 2.4039,
      "step": 2800000
    },
    {
      "epoch": 10.87,
      "grad_norm": 3.393655776977539,
      "learning_rate": 1.3789570434309568e-05,
      "loss": 2.4024,
      "step": 2810000
    },
    {
      "epoch": 10.91,
      "grad_norm": 3.257145881652832,
      "learning_rate": 1.3660738110983493e-05,
      "loss": 2.4031,
      "step": 2820000
    },
    {
      "epoch": 10.94,
      "grad_norm": 3.6838667392730713,
      "learning_rate": 1.3531854223789765e-05,
      "loss": 2.4021,
      "step": 2830000
    },
    {
      "epoch": 10.98,
      "grad_norm": 3.5828731060028076,
      "learning_rate": 1.3403034791430603e-05,
      "loss": 2.4025,
      "step": 2840000
    },
    {
      "epoch": 11.02,
      "grad_norm": 3.6322643756866455,
      "learning_rate": 1.3274138013269963e-05,
      "loss": 2.3981,
      "step": 2850000
    },
    {
      "epoch": 11.06,
      "grad_norm": 3.54144024848938,
      "learning_rate": 1.3145279908010062e-05,
      "loss": 2.395,
      "step": 2860000
    },
    {
      "epoch": 11.1,
      "grad_norm": 3.389280319213867,
      "learning_rate": 1.3016408911783246e-05,
      "loss": 2.3942,
      "step": 2870000
    },
    {
      "epoch": 11.14,
      "grad_norm": 3.532914876937866,
      "learning_rate": 1.2887550806523346e-05,
      "loss": 2.397,
      "step": 2880000
    },
    {
      "epoch": 11.18,
      "grad_norm": 3.4739973545074463,
      "learning_rate": 1.2758705592230359e-05,
      "loss": 2.3945,
      "step": 2890000
    },
    {
      "epoch": 11.22,
      "grad_norm": 3.585951805114746,
      "learning_rate": 1.2629821705036628e-05,
      "loss": 2.3954,
      "step": 2900000
    },
    {
      "epoch": 11.25,
      "grad_norm": 3.7103631496429443,
      "learning_rate": 1.250096359977673e-05,
      "loss": 2.392,
      "step": 2910000
    },
    {
      "epoch": 11.29,
      "grad_norm": 3.6249966621398926,
      "learning_rate": 1.2372105494516828e-05,
      "loss": 2.3911,
      "step": 2920000
    },
    {
      "epoch": 11.33,
      "grad_norm": 3.634864091873169,
      "learning_rate": 1.2243273171190752e-05,
      "loss": 2.3932,
      "step": 2930000
    },
    {
      "epoch": 11.37,
      "grad_norm": 3.5596530437469482,
      "learning_rate": 1.211441506593085e-05,
      "loss": 2.3921,
      "step": 2940000
    },
    {
      "epoch": 11.41,
      "grad_norm": 3.7196695804595947,
      "learning_rate": 1.1985544069704036e-05,
      "loss": 2.3916,
      "step": 2950000
    },
    {
      "epoch": 11.45,
      "grad_norm": 3.276970386505127,
      "learning_rate": 1.1856698855411049e-05,
      "loss": 2.3908,
      "step": 2960000
    },
    {
      "epoch": 11.49,
      "grad_norm": 3.504209280014038,
      "learning_rate": 1.1727840750151148e-05,
      "loss": 2.3918,
      "step": 2970000
    },
    {
      "epoch": 11.52,
      "grad_norm": 3.356379747390747,
      "learning_rate": 1.159895686295742e-05,
      "loss": 2.3882,
      "step": 2980000
    },
    {
      "epoch": 11.56,
      "grad_norm": 3.4934942722320557,
      "learning_rate": 1.1470085866730606e-05,
      "loss": 2.39,
      "step": 2990000
    },
    {
      "epoch": 11.6,
      "grad_norm": 3.5195186138153076,
      "learning_rate": 1.1341214870503792e-05,
      "loss": 2.3862,
      "step": 3000000
    },
    {
      "epoch": 11.64,
      "grad_norm": 3.470039129257202,
      "learning_rate": 1.1212343874276978e-05,
      "loss": 2.386,
      "step": 3010000
    },
    {
      "epoch": 11.68,
      "grad_norm": 3.560875177383423,
      "learning_rate": 1.1083511550950902e-05,
      "loss": 2.3844,
      "step": 3020000
    },
    {
      "epoch": 11.72,
      "grad_norm": 3.642742872238159,
      "learning_rate": 1.0954666336657914e-05,
      "loss": 2.3884,
      "step": 3030000
    },
    {
      "epoch": 11.76,
      "grad_norm": 3.634220838546753,
      "learning_rate": 1.0825769558497275e-05,
      "loss": 2.3841,
      "step": 3040000
    },
    {
      "epoch": 11.8,
      "grad_norm": 3.616865873336792,
      "learning_rate": 1.069689856227046e-05,
      "loss": 2.3842,
      "step": 3050000
    },
    {
      "epoch": 11.83,
      "grad_norm": 3.44041109085083,
      "learning_rate": 1.0568040457010559e-05,
      "loss": 2.384,
      "step": 3060000
    },
    {
      "epoch": 11.87,
      "grad_norm": 3.7081820964813232,
      "learning_rate": 1.0439182351750658e-05,
      "loss": 2.3832,
      "step": 3070000
    },
    {
      "epoch": 11.91,
      "grad_norm": 3.5147950649261475,
      "learning_rate": 1.0310324246490758e-05,
      "loss": 2.3832,
      "step": 3080000
    }
  ],
  "logging_steps": 10000,
  "max_steps": 3878685,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 20000,
  "total_flos": 1.0381966341677325e+20,
  "train_batch_size": 256,
  "trial_name": null,
  "trial_params": null
}
